# -*- coding: utf-8 -*-
"""AE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eeciO39C2aUfnb8GjzuAEgSp1YxWA_MK
"""

import numpy as np
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import torchvision.utils as vutils

from google.colab import drive
drive.mount('/content/gdrive')

!unzip '/content/gdrive/My Drive/APS360/Proj/face_data.zip'

# we might need to use this in the future 
# for now ignore it
!pip install split-folders
import split_folders

# Split with a ratio.
# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.
split_folders.ratio('./face_data', output="./output", seed=1337, ratio=(.8, .2)) # default values

# Number of GPUs available. Use 0 for CPU mode.
ngpu = 1
# Decide which device we want to run on
device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")

class AE3(nn.Module):
    def __init__(self, nc, ndf):
        super(AE3, self).__init__()
        self.name = "AE3"

        # encoder
        self.encoder = nn.Sequential(
          nn.Conv2d(nc, ndf, kernel_size=4, stride=2, padding=1),
          nn.ReLU(),
          nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1),
          nn.ReLU(),
          nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)
        )    

        # decoder
        self.decoder = nn.Sequential(
          nn.ConvTranspose2d(ndf*4, ndf*2, kernel_size=4, stride=2, padding=1),
          nn.ReLU(),
          nn.ConvTranspose2d(ndf*2, ndf, kernel_size=4, stride=2, padding=1),
          nn.ReLU(),
          nn.ConvTranspose2d(ndf, nc, kernel_size=4, stride=2, padding=1), 
          nn.Tanh()
        )
        
    def forward(self, x):
        emb = self.encoder(x)
        out = self.decoder(emb)
        return out

def get_AE_data_loader(batch_size=64):
  transform=transforms.Compose([
                               transforms.RandomHorizontalFlip(p=0.5),
                               transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                           ])
  dataset = torchvision.datasets.ImageFolder(root='./face_data', transform=transform)
  train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=1, shuffle=True)

  return train_loader

def train(model, num_epochs=5, batch_size=64, learning_rate=1e-4):
    torch.manual_seed(42)
    criterion = nn.MSELoss() # mean square error loss
    optimizer = torch.optim.Adam(model.parameters(),
                                 lr=learning_rate, 
                                 weight_decay=1e-5)
    
    outputs = []
    train_loss = np.zeros(num_epochs)
    for epoch in range(num_epochs):
        total_loss = 0.0
        train_loader = get_AE_data_loader(batch_size=batch_size)
        for i, data in enumerate(train_loader):
            img, _ = data
            img = img.to(device)
            recon = model(img)
            loss = criterion(recon, img)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            total_loss += loss.item()
        total_loss = total_loss / (i+1)
        train_loss[epoch] = total_loss
        model_path = "model_{0}_bs{1}_lr{2}_epoch{3}" .format(model.name, batch_size, learning_rate, epoch)
        torch.save(model.state_dict(), model_path)
            
        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(total_loss)))
        outputs.append((epoch, img, recon))
    plt.plot(train_loss, label="Train")
    plt.xlabel("Epoch")
    plt.ylabel("Trainig Loss")
    plt.legend(loc='best')
    plt.show()
    return outputs

# train model 
model = AE3(nc=3, ndf=8)
model = model.to(device)
output = train(model, num_epochs=40, batch_size=64, learning_rate=1e-4)

# visualize reconstruction and ori_image at different epochs
imgs = output[39][1].detach()
recon = output[39][2].detach()
imgs = np.transpose(vutils.make_grid(imgs[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))
recon = np.transpose(vutils.make_grid(recon[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))
plt.subplot(1, 2, 1)
plt.imshow(imgs)
plt.subplot(1, 2, 2)
plt.imshow(recon)

# show samples of AE
def show_samples(sample_number = 5):
  train_loader = get_AE_data_loader(sample_number)
  for data, _ in train_loader:
    data = data.cuda()
    recon_emb = model.encoder(data)
    rad = torch.randn(sample_number, 32, 8, 8).cuda()
    mod_emb = recon_emb + rad 
    mod_emb =  model.decoder(mod_emb)
    recon_outputs = model.decoder(recon_emb)
    for i in range(sample_number):
      images = mod_emb
      recon_images = recon_outputs
      ori_images = data 
      image = np.transpose(vutils.make_grid(images[i].detach().to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))
      recon_image = np.transpose(vutils.make_grid(recon_images[i].detach().to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))
      ori_image = np.transpose(vutils.make_grid(ori_images[i].detach().to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))
      plt.subplot(3, sample_number, i+1)
      plt.imshow(image)  
      plt.subplot(3, sample_number, sample_number+i+1)
      plt.imshow(recon_image)
      plt.subplot(3, sample_number, sample_number*2+i+1)
      plt.imshow(ori_image)
    break

show_samples(sample_number = 5)

def generate_rand_sample():
  sample = torch.randn(1, 32, 8, 8)
  sample = sample.cuda()
  recon = model.decoder(sample)
  recon = np.transpose(vutils.make_grid(recon[0].detach().to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))
  plt.imshow(recon)

