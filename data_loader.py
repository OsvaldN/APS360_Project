# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YScQ0WYR_I35xMvF1vjRo5RHlADfJY8H
"""

import numpy as np
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive')
!unzip '/content/gdrive/My Drive/face_data.zip'

# we might need to use this in the future 
# for now ignore it
pip install split-folders
import split_folders

# Split with a ratio.
# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.
split_folders.ratio('./face_data', output="./output", seed=1337, ratio=(.8, .2)) # default values

def get_data_loader(batch_size=64):
#pixel intensity in range -1 to 1
  transform = transforms.Compose(
          [transforms.RandomRotation(5),
           transforms.RandomHorizontalFlip(p=0.5),
           transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),
           transforms.ToTensor(),
           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

  dataset = torchvision.datasets.ImageFolder(root='./face_data', transform=transform)
  train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, num_workers=1, shuffle=True)

  return train_loader

train_loader = get_data_loader(batch_size=64)
k = 0
for images, labels in train_loader:
    image = images[0]
    # place the colour channel at the end, instead of at the beginning
    img = np.transpose(image, [1,2,0])
    # normalize pixel intensity values to [0, 1]
    img = img / 2 + 0.5
    plt.subplot(3, 5, k+1)
    plt.axis('off')
    plt.imshow(img)  
    k += 1
    if k > 14:
        break